{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGNMENT : IRIS MULTI-CLASS CLASSIFICATION\n",
    "\n",
    "###### Purpose :\n",
    "To predict the species of flower .\n",
    "###### Description :\n",
    "The dataset contains a set of 150 records under 5 attributes - Petal Length, Petal Width, Sepal Length, Sepal width and Class(Species).\n",
    "###### Requirements :\n",
    "1) Code must be in tf 2.0 .\n",
    "\n",
    "2) Accuracy must be in between 95-97% .\n",
    "\n",
    "3) Model shouldn't be Overfit (You can add drop out layer for this) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1 : Load all the necessary libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/ash316/ml-from-scratch-with-iris <br>\n",
    "http://tekshinobi.com/analyzing-iris-dataset-with-keras-and-tensorflow-machine-learning-and-data-analysis/ <br>\n",
    "https://machinelearningmastery.com/visualize-deep-learning-neural-network-model-keras/ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any results you write to the current directory are saved as output.\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2 : Data Preparation\n",
    "This step consists of multiple sub steps from data loading [download](https://github.com/ramsha275/PIAIC-Sir-Anees-Quarter-2/blob/master/Deep%20Learning/iris.csv),shuffling ,spliting in **Train** and **Test** sets to one-hot-enconding on labels . \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create Dataframes for features and target</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = keras.layers\n",
    "models = keras.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal.length  sepal.width  petal.length  petal.width variety\n",
       "0           5.1          3.5           1.4          0.2  Setosa\n",
       "1           4.9          3.0           1.4          0.2  Setosa\n",
       "2           4.7          3.2           1.3          0.2  Setosa\n",
       "3           4.6          3.1           1.5          0.2  Setosa\n",
       "4           5.0          3.6           1.4          0.2  Setosa"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df = pd.read_csv('iris.csv')\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal.length  sepal.width  petal.length  petal.width\n",
       "0           5.1          3.5           1.4          0.2\n",
       "1           4.9          3.0           1.4          0.2\n",
       "2           4.7          3.2           1.3          0.2\n",
       "3           4.6          3.1           1.5          0.2\n",
       "4           5.0          3.6           1.4          0.2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data_df = iris_df.loc[:, iris_df.columns != 'variety']\n",
    "iris_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variety = ['Setosa', 'Versicolor', 'Virginica']\n",
    "iris_target_array = iris_df['variety'].apply(variety.index)\n",
    "iris_target = np.array(iris_target_array)\n",
    "iris_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#target = encoder.fit_transform(iris_target)\n",
    "target = to_categorical(iris_target)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Setosa</th>\n",
       "      <th>Versicolor</th>\n",
       "      <th>Virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Setosa  Versicolor  Virginica\n",
       "0     1.0         0.0        0.0\n",
       "1     1.0         0.0        0.0\n",
       "2     1.0         0.0        0.0\n",
       "3     1.0         0.0        0.0\n",
       "4     1.0         0.0        0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iris_target_df = pd.DataFrame({\n",
    "#     'Setosa' : iris_df['variety'] == 'Setosa',\n",
    "#     'Versicolor' : iris_df['variety'] == 'Versicolor',\n",
    "#     'Virginica' : iris_df['variety'] == 'Virginica',\n",
    "# })*1\n",
    "# iris_target_df.head()\n",
    "iris_target_df = pd.DataFrame(data=target, columns=variety) \n",
    "iris_target_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create training and testing datasets</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(iris_data_df,\n",
    "                                                 iris_target_df,\n",
    "                                                 test_size=0.20,\n",
    "                                                 random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "30\n",
      "120\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n",
      "(30, 4)\n",
      "(120, 3)\n",
      "(30, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Feature scaling.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML algos perform best when all dataset features have the same scale. Particularly, in Neural Nets, we use MinMaxScaler with range between 0 and 1.<br>\n",
    "Also, since MinMaxScalar transformation makes us lose the column and index labels, we need to recreate the dataframes for scaled data and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler(feature_range=(0,1))\n",
    "# scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = pd.DataFrame(scaler.fit_transform(X_train),\n",
    "#                                columns=X_train.columns,\n",
    "#                                index=X_train.index)\n",
    "# X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = pd.DataFrame(scaler.transform(X_test),\n",
    "#                            columns=X_test.columns,\n",
    "#                            index=X_test.index)\n",
    "# X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Optional micro detail:</h5>\n",
    "Note that we use <b>fit_transform</b> on X_train and <b>transform</b> on X_test.<br>\n",
    "It is very important that we donâ€™t fit on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3 : Model Architecture \n",
    "\n",
    "\n",
    "###### Input : 4 \n",
    "###### 1 hidden Layer : 8 nodes\n",
    "###### Output : 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Build Model </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>tanh:</b><br>\n",
    "The exponential linear activation:\n",
    "<code>x</code>\n",
    "if\n",
    "<code>x>0</code>\n",
    "and\n",
    "<code>alpha * (exp(x)-1) </code>\n",
    "if\n",
    "<code>x<0</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# ##model.add(Dense(4, input_dim=4, activation='tanh', name='input_layer'))\n",
    "# model.add(Dense(4, input_shape=(4,), name='input_layer'))\n",
    "# model.add(Dense(8, activation='tanh', name='hidden_layer'))\n",
    "# model.add(layers.Activation('relu'))\n",
    "# model.add(Dense(3, activation='softmax', name='output_layer'))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (Dense)          (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "hidden_layer (Dense)         (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 3)                 27        \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 87\n",
      "Trainable params: 87\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(4, input_shape=(4,), name='input_layer'))\n",
    "model.add(Dense(8, activation='tanh', name='hidden_layer'))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(3, name='output_layer'))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4 : Compilation Step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (Dense)          (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "hidden_layer (Dense)         (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 3)                 27        \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 87\n",
      "Trainable params: 87\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sepal.length  sepal.width  petal.length  petal.width\n",
       "22           4.6          3.6           1.0          0.2\n",
       "15           5.7          4.4           1.5          0.4\n",
       "65           6.7          3.1           4.4          1.4\n",
       "11           4.8          3.4           1.6          0.2\n",
       "42           4.4          3.2           1.3          0.2"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>6.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal.length  sepal.width  petal.length  petal.width\n",
       "73            6.1          2.8           4.7          1.2\n",
       "18            5.7          3.8           1.7          0.3\n",
       "118           7.7          2.6           6.9          2.3\n",
       "78            6.0          2.9           4.5          1.5\n",
       "76            6.8          2.8           4.8          1.4"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model():\n",
    "#     \"\"\"build the Keras model callback\"\"\"\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(4, input_dim=4, activation='tanh', name='input_layer'))\n",
    "#     model.add(Dense(8, activation='tanh', name='hidden_layer'))\n",
    "#     model.add(Dense(3, activation='softmax', name='output_layer'))\n",
    " \n",
    "#     model.compile(loss=\"categorical_crossentropy\",\n",
    "#                   optimizer=\"adam\",\n",
    "#                   metrics=['accuracy'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create estimator</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator = KerasClassifier(\n",
    "#     build_fn=model,\n",
    "#     epochs=200, batch_size=20,\n",
    "#     verbose=2)\n",
    "# estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold = KFold(n_splits=5, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = cross_val_score(estimator, X_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"Model Performance: mean: %.2f%% std: (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tanh : Model Performance: mean: 86.67% std: (8.19%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 108 samples, validate on 12 samples\n",
      "Epoch 1/200\n",
      "108/108 - 3s - loss: 1.1804 - acc: 0.3148 - val_loss: 1.1049 - val_acc: 0.4167\n",
      "Epoch 2/200\n",
      "108/108 - 0s - loss: 1.1607 - acc: 0.3148 - val_loss: 1.0962 - val_acc: 0.4167\n",
      "Epoch 3/200\n",
      "108/108 - 0s - loss: 1.1440 - acc: 0.3148 - val_loss: 1.0869 - val_acc: 0.4167\n",
      "Epoch 4/200\n",
      "108/108 - 0s - loss: 1.1234 - acc: 0.3148 - val_loss: 1.0773 - val_acc: 0.4167\n",
      "Epoch 5/200\n",
      "108/108 - 0s - loss: 1.1002 - acc: 0.3148 - val_loss: 1.0666 - val_acc: 0.4167\n",
      "Epoch 6/200\n",
      "108/108 - 0s - loss: 1.0774 - acc: 0.3148 - val_loss: 1.0566 - val_acc: 0.4167\n",
      "Epoch 7/200\n",
      "108/108 - 0s - loss: 1.0518 - acc: 0.3241 - val_loss: 1.0460 - val_acc: 0.4167\n",
      "Epoch 8/200\n",
      "108/108 - 0s - loss: 1.0220 - acc: 0.3889 - val_loss: 1.0350 - val_acc: 0.4167\n",
      "Epoch 9/200\n",
      "108/108 - 0s - loss: 0.9939 - acc: 0.5185 - val_loss: 1.0235 - val_acc: 0.4167\n",
      "Epoch 10/200\n",
      "108/108 - 0s - loss: 0.9637 - acc: 0.6204 - val_loss: 1.0108 - val_acc: 0.5833\n",
      "Epoch 11/200\n",
      "108/108 - 0s - loss: 0.9382 - acc: 0.6574 - val_loss: 0.9962 - val_acc: 0.5833\n",
      "Epoch 12/200\n",
      "108/108 - 0s - loss: 0.9121 - acc: 0.6574 - val_loss: 0.9803 - val_acc: 0.5833\n",
      "Epoch 13/200\n",
      "108/108 - 0s - loss: 0.8900 - acc: 0.6389 - val_loss: 0.9630 - val_acc: 0.5000\n",
      "Epoch 14/200\n",
      "108/108 - 0s - loss: 0.8648 - acc: 0.5463 - val_loss: 0.9439 - val_acc: 0.1667\n",
      "Epoch 15/200\n",
      "108/108 - 0s - loss: 0.8420 - acc: 0.4815 - val_loss: 0.9236 - val_acc: 0.1667\n",
      "Epoch 16/200\n",
      "108/108 - 0s - loss: 0.8217 - acc: 0.4167 - val_loss: 0.9011 - val_acc: 0.2500\n",
      "Epoch 17/200\n",
      "108/108 - 0s - loss: 0.8000 - acc: 0.5370 - val_loss: 0.8779 - val_acc: 0.4167\n",
      "Epoch 18/200\n",
      "108/108 - 0s - loss: 0.7784 - acc: 0.6574 - val_loss: 0.8545 - val_acc: 0.5833\n",
      "Epoch 19/200\n",
      "108/108 - 0s - loss: 0.7580 - acc: 0.6667 - val_loss: 0.8311 - val_acc: 0.5833\n",
      "Epoch 20/200\n",
      "108/108 - 0s - loss: 0.7384 - acc: 0.6852 - val_loss: 0.8093 - val_acc: 0.5833\n",
      "Epoch 21/200\n",
      "108/108 - 0s - loss: 0.7196 - acc: 0.6852 - val_loss: 0.7904 - val_acc: 0.5833\n",
      "Epoch 22/200\n",
      "108/108 - 0s - loss: 0.7033 - acc: 0.6852 - val_loss: 0.7734 - val_acc: 0.5833\n",
      "Epoch 23/200\n",
      "108/108 - 0s - loss: 0.6881 - acc: 0.6852 - val_loss: 0.7588 - val_acc: 0.5833\n",
      "Epoch 24/200\n",
      "108/108 - 0s - loss: 0.6748 - acc: 0.6852 - val_loss: 0.7461 - val_acc: 0.5833\n",
      "Epoch 25/200\n",
      "108/108 - 0s - loss: 0.6628 - acc: 0.6852 - val_loss: 0.7349 - val_acc: 0.5833\n",
      "Epoch 26/200\n",
      "108/108 - 0s - loss: 0.6514 - acc: 0.6852 - val_loss: 0.7250 - val_acc: 0.5833\n",
      "Epoch 27/200\n",
      "108/108 - 0s - loss: 0.6406 - acc: 0.7407 - val_loss: 0.7158 - val_acc: 0.5833\n",
      "Epoch 28/200\n",
      "108/108 - 0s - loss: 0.6303 - acc: 0.7870 - val_loss: 0.7074 - val_acc: 0.6667\n",
      "Epoch 29/200\n",
      "108/108 - 0s - loss: 0.6200 - acc: 0.8241 - val_loss: 0.6996 - val_acc: 0.6667\n",
      "Epoch 30/200\n",
      "108/108 - 0s - loss: 0.6107 - acc: 0.8704 - val_loss: 0.6924 - val_acc: 0.7500\n",
      "Epoch 31/200\n",
      "108/108 - 0s - loss: 0.6018 - acc: 0.8981 - val_loss: 0.6854 - val_acc: 0.8333\n",
      "Epoch 32/200\n",
      "108/108 - 0s - loss: 0.5937 - acc: 0.9444 - val_loss: 0.6790 - val_acc: 0.9167\n",
      "Epoch 33/200\n",
      "108/108 - 0s - loss: 0.5855 - acc: 0.9815 - val_loss: 0.6732 - val_acc: 0.9167\n",
      "Epoch 34/200\n",
      "108/108 - 0s - loss: 0.5785 - acc: 0.9537 - val_loss: 0.6675 - val_acc: 0.9167\n",
      "Epoch 35/200\n",
      "108/108 - 0s - loss: 0.5711 - acc: 0.9630 - val_loss: 0.6621 - val_acc: 0.9167\n",
      "Epoch 36/200\n",
      "108/108 - 0s - loss: 0.5646 - acc: 0.9444 - val_loss: 0.6571 - val_acc: 0.9167\n",
      "Epoch 37/200\n",
      "108/108 - 0s - loss: 0.5586 - acc: 0.9352 - val_loss: 0.6525 - val_acc: 0.9167\n",
      "Epoch 38/200\n",
      "108/108 - 0s - loss: 0.5529 - acc: 0.9259 - val_loss: 0.6482 - val_acc: 0.9167\n",
      "Epoch 39/200\n",
      "108/108 - 0s - loss: 0.5474 - acc: 0.9167 - val_loss: 0.6438 - val_acc: 0.9167\n",
      "Epoch 40/200\n",
      "108/108 - 0s - loss: 0.5423 - acc: 0.8796 - val_loss: 0.6397 - val_acc: 0.9167\n",
      "Epoch 41/200\n",
      "108/108 - 0s - loss: 0.5373 - acc: 0.8796 - val_loss: 0.6358 - val_acc: 0.9167\n",
      "Epoch 42/200\n",
      "108/108 - 0s - loss: 0.5328 - acc: 0.8796 - val_loss: 0.6322 - val_acc: 0.9167\n",
      "Epoch 43/200\n",
      "108/108 - 0s - loss: 0.5283 - acc: 0.8796 - val_loss: 0.6285 - val_acc: 0.9167\n",
      "Epoch 44/200\n",
      "108/108 - 0s - loss: 0.5241 - acc: 0.8796 - val_loss: 0.6254 - val_acc: 0.9167\n",
      "Epoch 45/200\n",
      "108/108 - 0s - loss: 0.5199 - acc: 0.8796 - val_loss: 0.6223 - val_acc: 0.9167\n",
      "Epoch 46/200\n",
      "108/108 - 0s - loss: 0.5160 - acc: 0.8796 - val_loss: 0.6193 - val_acc: 0.9167\n",
      "Epoch 47/200\n",
      "108/108 - 0s - loss: 0.5122 - acc: 0.8796 - val_loss: 0.6161 - val_acc: 0.9167\n",
      "Epoch 48/200\n",
      "108/108 - 0s - loss: 0.5086 - acc: 0.8796 - val_loss: 0.6133 - val_acc: 0.9167\n",
      "Epoch 49/200\n",
      "108/108 - 0s - loss: 0.5050 - acc: 0.8796 - val_loss: 0.6105 - val_acc: 0.9167\n",
      "Epoch 50/200\n",
      "108/108 - 0s - loss: 0.5017 - acc: 0.8796 - val_loss: 0.6077 - val_acc: 0.9167\n",
      "Epoch 51/200\n",
      "108/108 - 0s - loss: 0.4984 - acc: 0.8981 - val_loss: 0.6052 - val_acc: 0.9167\n",
      "Epoch 52/200\n",
      "108/108 - 0s - loss: 0.4951 - acc: 0.9074 - val_loss: 0.6029 - val_acc: 0.9167\n",
      "Epoch 53/200\n",
      "108/108 - 0s - loss: 0.4920 - acc: 0.9167 - val_loss: 0.6008 - val_acc: 0.9167\n",
      "Epoch 54/200\n",
      "108/108 - 0s - loss: 0.4890 - acc: 0.9259 - val_loss: 0.5987 - val_acc: 0.9167\n",
      "Epoch 55/200\n",
      "108/108 - 0s - loss: 0.4861 - acc: 0.9352 - val_loss: 0.5967 - val_acc: 0.9167\n",
      "Epoch 56/200\n",
      "108/108 - 0s - loss: 0.4834 - acc: 0.9444 - val_loss: 0.5944 - val_acc: 0.9167\n",
      "Epoch 57/200\n",
      "108/108 - 0s - loss: 0.4809 - acc: 0.9630 - val_loss: 0.5924 - val_acc: 0.9167\n",
      "Epoch 58/200\n",
      "108/108 - 0s - loss: 0.4781 - acc: 0.9630 - val_loss: 0.5903 - val_acc: 0.9167\n",
      "Epoch 59/200\n",
      "108/108 - 0s - loss: 0.4755 - acc: 0.9630 - val_loss: 0.5883 - val_acc: 0.9167\n",
      "Epoch 60/200\n",
      "108/108 - 0s - loss: 0.4729 - acc: 0.9630 - val_loss: 0.5860 - val_acc: 0.9167\n",
      "Epoch 61/200\n",
      "108/108 - 0s - loss: 0.4705 - acc: 0.9537 - val_loss: 0.5835 - val_acc: 0.9167\n",
      "Epoch 62/200\n",
      "108/108 - 0s - loss: 0.4676 - acc: 0.9444 - val_loss: 0.5812 - val_acc: 0.9167\n",
      "Epoch 63/200\n",
      "108/108 - 0s - loss: 0.4652 - acc: 0.9352 - val_loss: 0.5789 - val_acc: 0.9167\n",
      "Epoch 64/200\n",
      "108/108 - 0s - loss: 0.4628 - acc: 0.9352 - val_loss: 0.5768 - val_acc: 0.9167\n",
      "Epoch 65/200\n",
      "108/108 - 0s - loss: 0.4603 - acc: 0.9352 - val_loss: 0.5749 - val_acc: 0.9167\n",
      "Epoch 66/200\n",
      "108/108 - 0s - loss: 0.4580 - acc: 0.9352 - val_loss: 0.5730 - val_acc: 0.9167\n",
      "Epoch 67/200\n",
      "108/108 - 0s - loss: 0.4557 - acc: 0.9352 - val_loss: 0.5712 - val_acc: 0.9167\n",
      "Epoch 68/200\n",
      "108/108 - 0s - loss: 0.4534 - acc: 0.9352 - val_loss: 0.5689 - val_acc: 0.9167\n",
      "Epoch 69/200\n",
      "108/108 - 0s - loss: 0.4511 - acc: 0.9352 - val_loss: 0.5669 - val_acc: 0.9167\n",
      "Epoch 70/200\n",
      "108/108 - 0s - loss: 0.4491 - acc: 0.9352 - val_loss: 0.5649 - val_acc: 0.9167\n",
      "Epoch 71/200\n",
      "108/108 - 0s - loss: 0.4468 - acc: 0.9352 - val_loss: 0.5630 - val_acc: 0.9167\n",
      "Epoch 72/200\n",
      "108/108 - 0s - loss: 0.4446 - acc: 0.9352 - val_loss: 0.5614 - val_acc: 0.9167\n",
      "Epoch 73/200\n",
      "108/108 - 0s - loss: 0.4424 - acc: 0.9352 - val_loss: 0.5597 - val_acc: 0.9167\n",
      "Epoch 74/200\n",
      "108/108 - 0s - loss: 0.4402 - acc: 0.9444 - val_loss: 0.5580 - val_acc: 0.9167\n",
      "Epoch 75/200\n",
      "108/108 - 0s - loss: 0.4381 - acc: 0.9537 - val_loss: 0.5562 - val_acc: 0.9167\n",
      "Epoch 76/200\n",
      "108/108 - 0s - loss: 0.4359 - acc: 0.9537 - val_loss: 0.5546 - val_acc: 0.9167\n",
      "Epoch 77/200\n",
      "108/108 - 0s - loss: 0.4339 - acc: 0.9537 - val_loss: 0.5529 - val_acc: 0.9167\n",
      "Epoch 78/200\n",
      "108/108 - 0s - loss: 0.4316 - acc: 0.9630 - val_loss: 0.5511 - val_acc: 0.9167\n",
      "Epoch 79/200\n",
      "108/108 - 0s - loss: 0.4295 - acc: 0.9630 - val_loss: 0.5492 - val_acc: 0.9167\n",
      "Epoch 80/200\n",
      "108/108 - 0s - loss: 0.4273 - acc: 0.9537 - val_loss: 0.5473 - val_acc: 0.9167\n",
      "Epoch 81/200\n",
      "108/108 - 0s - loss: 0.4253 - acc: 0.9630 - val_loss: 0.5456 - val_acc: 0.9167\n",
      "Epoch 82/200\n",
      "108/108 - 0s - loss: 0.4230 - acc: 0.9630 - val_loss: 0.5437 - val_acc: 0.9167\n",
      "Epoch 83/200\n",
      "108/108 - 0s - loss: 0.4208 - acc: 0.9630 - val_loss: 0.5416 - val_acc: 0.9167\n",
      "Epoch 84/200\n",
      "108/108 - 0s - loss: 0.4188 - acc: 0.9630 - val_loss: 0.5394 - val_acc: 0.9167\n",
      "Epoch 85/200\n",
      "108/108 - 0s - loss: 0.4166 - acc: 0.9630 - val_loss: 0.5375 - val_acc: 0.9167\n",
      "Epoch 86/200\n",
      "108/108 - 0s - loss: 0.4145 - acc: 0.9630 - val_loss: 0.5355 - val_acc: 0.9167\n",
      "Epoch 87/200\n",
      "108/108 - 0s - loss: 0.4125 - acc: 0.9537 - val_loss: 0.5333 - val_acc: 0.9167\n",
      "Epoch 88/200\n",
      "108/108 - 0s - loss: 0.4102 - acc: 0.9630 - val_loss: 0.5317 - val_acc: 0.9167\n",
      "Epoch 89/200\n",
      "108/108 - 0s - loss: 0.4083 - acc: 0.9630 - val_loss: 0.5298 - val_acc: 0.9167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/200\n",
      "108/108 - 0s - loss: 0.4061 - acc: 0.9630 - val_loss: 0.5271 - val_acc: 0.9167\n",
      "Epoch 91/200\n",
      "108/108 - 0s - loss: 0.4037 - acc: 0.9630 - val_loss: 0.5246 - val_acc: 0.9167\n",
      "Epoch 92/200\n",
      "108/108 - 0s - loss: 0.4019 - acc: 0.9537 - val_loss: 0.5222 - val_acc: 0.9167\n",
      "Epoch 93/200\n",
      "108/108 - 0s - loss: 0.3996 - acc: 0.9537 - val_loss: 0.5200 - val_acc: 0.9167\n",
      "Epoch 94/200\n",
      "108/108 - 0s - loss: 0.3981 - acc: 0.9630 - val_loss: 0.5180 - val_acc: 0.9167\n",
      "Epoch 95/200\n",
      "108/108 - 0s - loss: 0.3954 - acc: 0.9630 - val_loss: 0.5156 - val_acc: 0.9167\n",
      "Epoch 96/200\n",
      "108/108 - 0s - loss: 0.3932 - acc: 0.9630 - val_loss: 0.5134 - val_acc: 0.9167\n",
      "Epoch 97/200\n",
      "108/108 - 0s - loss: 0.3911 - acc: 0.9630 - val_loss: 0.5112 - val_acc: 0.9167\n",
      "Epoch 98/200\n",
      "108/108 - 0s - loss: 0.3888 - acc: 0.9630 - val_loss: 0.5093 - val_acc: 0.9167\n",
      "Epoch 99/200\n",
      "108/108 - 0s - loss: 0.3867 - acc: 0.9630 - val_loss: 0.5074 - val_acc: 0.9167\n",
      "Epoch 100/200\n",
      "108/108 - 0s - loss: 0.3844 - acc: 0.9630 - val_loss: 0.5055 - val_acc: 0.9167\n",
      "Epoch 101/200\n",
      "108/108 - 0s - loss: 0.3826 - acc: 0.9722 - val_loss: 0.5038 - val_acc: 0.9167\n",
      "Epoch 102/200\n",
      "108/108 - 0s - loss: 0.3802 - acc: 0.9722 - val_loss: 0.5010 - val_acc: 0.9167\n",
      "Epoch 103/200\n",
      "108/108 - 0s - loss: 0.3782 - acc: 0.9722 - val_loss: 0.4975 - val_acc: 0.9167\n",
      "Epoch 104/200\n",
      "108/108 - 0s - loss: 0.3758 - acc: 0.9630 - val_loss: 0.4946 - val_acc: 0.9167\n",
      "Epoch 105/200\n",
      "108/108 - 0s - loss: 0.3733 - acc: 0.9630 - val_loss: 0.4921 - val_acc: 0.9167\n",
      "Epoch 106/200\n",
      "108/108 - 0s - loss: 0.3711 - acc: 0.9630 - val_loss: 0.4895 - val_acc: 0.9167\n",
      "Epoch 107/200\n",
      "108/108 - 0s - loss: 0.3687 - acc: 0.9630 - val_loss: 0.4870 - val_acc: 0.9167\n",
      "Epoch 108/200\n",
      "108/108 - 0s - loss: 0.3663 - acc: 0.9630 - val_loss: 0.4843 - val_acc: 0.9167\n",
      "Epoch 109/200\n",
      "108/108 - 0s - loss: 0.3639 - acc: 0.9630 - val_loss: 0.4816 - val_acc: 0.9167\n",
      "Epoch 110/200\n",
      "108/108 - 0s - loss: 0.3617 - acc: 0.9722 - val_loss: 0.4793 - val_acc: 0.9167\n",
      "Epoch 111/200\n",
      "108/108 - 0s - loss: 0.3591 - acc: 0.9722 - val_loss: 0.4770 - val_acc: 0.9167\n",
      "Epoch 112/200\n",
      "108/108 - 0s - loss: 0.3567 - acc: 0.9722 - val_loss: 0.4743 - val_acc: 0.9167\n",
      "Epoch 113/200\n",
      "108/108 - 0s - loss: 0.3544 - acc: 0.9722 - val_loss: 0.4716 - val_acc: 0.9167\n",
      "Epoch 114/200\n",
      "108/108 - 0s - loss: 0.3519 - acc: 0.9722 - val_loss: 0.4685 - val_acc: 0.9167\n",
      "Epoch 115/200\n",
      "108/108 - 0s - loss: 0.3495 - acc: 0.9722 - val_loss: 0.4652 - val_acc: 0.9167\n",
      "Epoch 116/200\n",
      "108/108 - 0s - loss: 0.3470 - acc: 0.9722 - val_loss: 0.4620 - val_acc: 0.9167\n",
      "Epoch 117/200\n",
      "108/108 - 0s - loss: 0.3447 - acc: 0.9630 - val_loss: 0.4588 - val_acc: 0.9167\n",
      "Epoch 118/200\n",
      "108/108 - 0s - loss: 0.3432 - acc: 0.9630 - val_loss: 0.4556 - val_acc: 0.9167\n",
      "Epoch 119/200\n",
      "108/108 - 0s - loss: 0.3400 - acc: 0.9630 - val_loss: 0.4532 - val_acc: 0.9167\n",
      "Epoch 120/200\n",
      "108/108 - 0s - loss: 0.3371 - acc: 0.9722 - val_loss: 0.4508 - val_acc: 0.9167\n",
      "Epoch 121/200\n",
      "108/108 - 0s - loss: 0.3346 - acc: 0.9722 - val_loss: 0.4485 - val_acc: 0.9167\n",
      "Epoch 122/200\n",
      "108/108 - 0s - loss: 0.3324 - acc: 0.9722 - val_loss: 0.4457 - val_acc: 0.9167\n",
      "Epoch 123/200\n",
      "108/108 - 0s - loss: 0.3301 - acc: 0.9722 - val_loss: 0.4426 - val_acc: 0.9167\n",
      "Epoch 124/200\n",
      "108/108 - 0s - loss: 0.3275 - acc: 0.9722 - val_loss: 0.4397 - val_acc: 0.9167\n",
      "Epoch 125/200\n",
      "108/108 - 0s - loss: 0.3248 - acc: 0.9722 - val_loss: 0.4355 - val_acc: 0.9167\n",
      "Epoch 126/200\n",
      "108/108 - 0s - loss: 0.3224 - acc: 0.9722 - val_loss: 0.4314 - val_acc: 0.9167\n",
      "Epoch 127/200\n",
      "108/108 - 0s - loss: 0.3199 - acc: 0.9722 - val_loss: 0.4281 - val_acc: 0.9167\n",
      "Epoch 128/200\n",
      "108/108 - 0s - loss: 0.3175 - acc: 0.9722 - val_loss: 0.4250 - val_acc: 0.9167\n",
      "Epoch 129/200\n",
      "108/108 - 0s - loss: 0.3150 - acc: 0.9722 - val_loss: 0.4219 - val_acc: 0.9167\n",
      "Epoch 130/200\n",
      "108/108 - 0s - loss: 0.3122 - acc: 0.9722 - val_loss: 0.4187 - val_acc: 0.9167\n",
      "Epoch 131/200\n",
      "108/108 - 0s - loss: 0.3096 - acc: 0.9722 - val_loss: 0.4156 - val_acc: 0.9167\n",
      "Epoch 132/200\n",
      "108/108 - 0s - loss: 0.3070 - acc: 0.9722 - val_loss: 0.4125 - val_acc: 0.9167\n",
      "Epoch 133/200\n",
      "108/108 - 0s - loss: 0.3048 - acc: 0.9722 - val_loss: 0.4101 - val_acc: 0.9167\n",
      "Epoch 134/200\n",
      "108/108 - 0s - loss: 0.3017 - acc: 0.9722 - val_loss: 0.4061 - val_acc: 0.9167\n",
      "Epoch 135/200\n",
      "108/108 - 0s - loss: 0.2992 - acc: 0.9722 - val_loss: 0.4024 - val_acc: 0.9167\n",
      "Epoch 136/200\n",
      "108/108 - 0s - loss: 0.2962 - acc: 0.9722 - val_loss: 0.4002 - val_acc: 0.9167\n",
      "Epoch 137/200\n",
      "108/108 - 0s - loss: 0.2941 - acc: 0.9722 - val_loss: 0.3972 - val_acc: 0.9167\n",
      "Epoch 138/200\n",
      "108/108 - 0s - loss: 0.2914 - acc: 0.9722 - val_loss: 0.3961 - val_acc: 1.0000\n",
      "Epoch 139/200\n",
      "108/108 - 0s - loss: 0.2898 - acc: 0.9630 - val_loss: 0.3934 - val_acc: 1.0000\n",
      "Epoch 140/200\n",
      "108/108 - 0s - loss: 0.2871 - acc: 0.9537 - val_loss: 0.3887 - val_acc: 1.0000\n",
      "Epoch 141/200\n",
      "108/108 - 0s - loss: 0.2847 - acc: 0.9722 - val_loss: 0.3818 - val_acc: 0.9167\n",
      "Epoch 142/200\n",
      "108/108 - 0s - loss: 0.2808 - acc: 0.9722 - val_loss: 0.3775 - val_acc: 0.9167\n",
      "Epoch 143/200\n",
      "108/108 - 0s - loss: 0.2787 - acc: 0.9722 - val_loss: 0.3736 - val_acc: 0.9167\n",
      "Epoch 144/200\n",
      "108/108 - 0s - loss: 0.2762 - acc: 0.9722 - val_loss: 0.3703 - val_acc: 0.9167\n",
      "Epoch 145/200\n",
      "108/108 - 0s - loss: 0.2733 - acc: 0.9722 - val_loss: 0.3670 - val_acc: 0.9167\n",
      "Epoch 146/200\n",
      "108/108 - 0s - loss: 0.2704 - acc: 0.9722 - val_loss: 0.3642 - val_acc: 0.9167\n",
      "Epoch 147/200\n",
      "108/108 - 0s - loss: 0.2675 - acc: 0.9722 - val_loss: 0.3632 - val_acc: 1.0000\n",
      "Epoch 148/200\n",
      "108/108 - 0s - loss: 0.2659 - acc: 0.9630 - val_loss: 0.3611 - val_acc: 1.0000\n",
      "Epoch 149/200\n",
      "108/108 - 0s - loss: 0.2634 - acc: 0.9537 - val_loss: 0.3565 - val_acc: 1.0000\n",
      "Epoch 150/200\n",
      "108/108 - 0s - loss: 0.2603 - acc: 0.9722 - val_loss: 0.3508 - val_acc: 1.0000\n",
      "Epoch 151/200\n",
      "108/108 - 0s - loss: 0.2577 - acc: 0.9722 - val_loss: 0.3462 - val_acc: 0.9167\n",
      "Epoch 152/200\n",
      "108/108 - 0s - loss: 0.2560 - acc: 0.9722 - val_loss: 0.3437 - val_acc: 1.0000\n",
      "Epoch 153/200\n",
      "108/108 - 0s - loss: 0.2526 - acc: 0.9722 - val_loss: 0.3389 - val_acc: 1.0000\n",
      "Epoch 154/200\n",
      "108/108 - 0s - loss: 0.2500 - acc: 0.9722 - val_loss: 0.3345 - val_acc: 0.9167\n",
      "Epoch 155/200\n",
      "108/108 - 0s - loss: 0.2482 - acc: 0.9722 - val_loss: 0.3307 - val_acc: 0.9167\n",
      "Epoch 156/200\n",
      "108/108 - 0s - loss: 0.2452 - acc: 0.9722 - val_loss: 0.3282 - val_acc: 1.0000\n",
      "Epoch 157/200\n",
      "108/108 - 0s - loss: 0.2425 - acc: 0.9722 - val_loss: 0.3270 - val_acc: 1.0000\n",
      "Epoch 158/200\n",
      "108/108 - 0s - loss: 0.2404 - acc: 0.9722 - val_loss: 0.3244 - val_acc: 1.0000\n",
      "Epoch 159/200\n",
      "108/108 - 0s - loss: 0.2377 - acc: 0.9722 - val_loss: 0.3192 - val_acc: 1.0000\n",
      "Epoch 160/200\n",
      "108/108 - 0s - loss: 0.2354 - acc: 0.9722 - val_loss: 0.3142 - val_acc: 1.0000\n",
      "Epoch 161/200\n",
      "108/108 - 0s - loss: 0.2327 - acc: 0.9722 - val_loss: 0.3111 - val_acc: 1.0000\n",
      "Epoch 162/200\n",
      "108/108 - 0s - loss: 0.2306 - acc: 0.9722 - val_loss: 0.3082 - val_acc: 1.0000\n",
      "Epoch 163/200\n",
      "108/108 - 0s - loss: 0.2279 - acc: 0.9722 - val_loss: 0.3070 - val_acc: 1.0000\n",
      "Epoch 164/200\n",
      "108/108 - 0s - loss: 0.2259 - acc: 0.9722 - val_loss: 0.3050 - val_acc: 1.0000\n",
      "Epoch 165/200\n",
      "108/108 - 0s - loss: 0.2239 - acc: 0.9722 - val_loss: 0.3019 - val_acc: 1.0000\n",
      "Epoch 166/200\n",
      "108/108 - 0s - loss: 0.2216 - acc: 0.9722 - val_loss: 0.2964 - val_acc: 1.0000\n",
      "Epoch 167/200\n",
      "108/108 - 0s - loss: 0.2196 - acc: 0.9722 - val_loss: 0.2916 - val_acc: 1.0000\n",
      "Epoch 168/200\n",
      "108/108 - 0s - loss: 0.2174 - acc: 0.9722 - val_loss: 0.2879 - val_acc: 1.0000\n",
      "Epoch 169/200\n",
      "108/108 - 0s - loss: 0.2149 - acc: 0.9722 - val_loss: 0.2855 - val_acc: 1.0000\n",
      "Epoch 170/200\n",
      "108/108 - 0s - loss: 0.2127 - acc: 0.9722 - val_loss: 0.2826 - val_acc: 1.0000\n",
      "Epoch 171/200\n",
      "108/108 - 0s - loss: 0.2104 - acc: 0.9722 - val_loss: 0.2810 - val_acc: 1.0000\n",
      "Epoch 172/200\n",
      "108/108 - 0s - loss: 0.2082 - acc: 0.9722 - val_loss: 0.2804 - val_acc: 1.0000\n",
      "Epoch 173/200\n",
      "108/108 - 0s - loss: 0.2070 - acc: 0.9722 - val_loss: 0.2788 - val_acc: 1.0000\n",
      "Epoch 174/200\n",
      "108/108 - 0s - loss: 0.2050 - acc: 0.9722 - val_loss: 0.2737 - val_acc: 1.0000\n",
      "Epoch 175/200\n",
      "108/108 - 0s - loss: 0.2024 - acc: 0.9815 - val_loss: 0.2694 - val_acc: 1.0000\n",
      "Epoch 176/200\n",
      "108/108 - 0s - loss: 0.2002 - acc: 0.9722 - val_loss: 0.2647 - val_acc: 1.0000\n",
      "Epoch 177/200\n",
      "108/108 - 0s - loss: 0.1985 - acc: 0.9722 - val_loss: 0.2605 - val_acc: 1.0000\n",
      "Epoch 178/200\n",
      "108/108 - 0s - loss: 0.1974 - acc: 0.9722 - val_loss: 0.2577 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/200\n",
      "108/108 - 0s - loss: 0.1954 - acc: 0.9722 - val_loss: 0.2556 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "108/108 - 0s - loss: 0.1931 - acc: 0.9722 - val_loss: 0.2537 - val_acc: 1.0000\n",
      "Epoch 181/200\n",
      "108/108 - 0s - loss: 0.1910 - acc: 0.9722 - val_loss: 0.2513 - val_acc: 1.0000\n",
      "Epoch 182/200\n",
      "108/108 - 0s - loss: 0.1888 - acc: 0.9722 - val_loss: 0.2509 - val_acc: 1.0000\n",
      "Epoch 183/200\n",
      "108/108 - 0s - loss: 0.1871 - acc: 0.9722 - val_loss: 0.2512 - val_acc: 1.0000\n",
      "Epoch 184/200\n",
      "108/108 - 0s - loss: 0.1864 - acc: 0.9630 - val_loss: 0.2508 - val_acc: 1.0000\n",
      "Epoch 185/200\n",
      "108/108 - 0s - loss: 0.1854 - acc: 0.9630 - val_loss: 0.2444 - val_acc: 1.0000\n",
      "Epoch 186/200\n",
      "108/108 - 0s - loss: 0.1828 - acc: 0.9722 - val_loss: 0.2419 - val_acc: 1.0000\n",
      "Epoch 187/200\n",
      "108/108 - 0s - loss: 0.1804 - acc: 0.9722 - val_loss: 0.2366 - val_acc: 1.0000\n",
      "Epoch 188/200\n",
      "108/108 - 0s - loss: 0.1788 - acc: 0.9815 - val_loss: 0.2317 - val_acc: 1.0000\n",
      "Epoch 189/200\n",
      "108/108 - 0s - loss: 0.1775 - acc: 0.9722 - val_loss: 0.2290 - val_acc: 1.0000\n",
      "Epoch 190/200\n",
      "108/108 - 0s - loss: 0.1765 - acc: 0.9722 - val_loss: 0.2262 - val_acc: 1.0000\n",
      "Epoch 191/200\n",
      "108/108 - 0s - loss: 0.1752 - acc: 0.9722 - val_loss: 0.2258 - val_acc: 1.0000\n",
      "Epoch 192/200\n",
      "108/108 - 0s - loss: 0.1726 - acc: 0.9722 - val_loss: 0.2238 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "108/108 - 0s - loss: 0.1708 - acc: 0.9722 - val_loss: 0.2200 - val_acc: 1.0000\n",
      "Epoch 194/200\n",
      "108/108 - 0s - loss: 0.1696 - acc: 0.9722 - val_loss: 0.2176 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "108/108 - 0s - loss: 0.1686 - acc: 0.9722 - val_loss: 0.2150 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "108/108 - 0s - loss: 0.1665 - acc: 0.9722 - val_loss: 0.2147 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "108/108 - 0s - loss: 0.1649 - acc: 0.9815 - val_loss: 0.2143 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "108/108 - 0s - loss: 0.1638 - acc: 0.9722 - val_loss: 0.2118 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "108/108 - 0s - loss: 0.1624 - acc: 0.9722 - val_loss: 0.2115 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "108/108 - 0s - loss: 0.1608 - acc: 0.9722 - val_loss: 0.2088 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#model = model()\n",
    "history = model.fit(\n",
    "       X_train,\n",
    "       y_train,\n",
    "       validation_split=.1,\n",
    "       epochs=200,\n",
    "       shuffle=True, # shuffle data randomly.\n",
    "       #NNs perform best on randomly shuffled data\n",
    "       verbose=2 # this will tell keras to print more detailed info\n",
    "       # during trainnig to know what is going on\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "activation=tanh\n",
    "epoch200 -> 100%acc\n",
    "epoch100 -> 100%acc\n",
    "epoch4 -> 30%acc\n",
    "epoch50 -> 87%acc\n",
    "epoch75 ->83%acc\n",
    "epoch90 ->90%acc\n",
    "epoch100 ->86.67%acc\n",
    "epoch100 ->90%acc\n",
    "epoch200 ->96.6%acc\n",
    "\n",
    "\n",
    "activation=relu\n",
    "epoch100 ->93%acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Optional micro detail:</h5>\n",
    "<ol>\n",
    "    <li>\n",
    "        Input layer(layer_1) has 4 inputs corresponding to 4 feature columns in X_train. <br> The output_layer has 3 outputs for the three classes in target. </li>\n",
    "    <li>\n",
    "        You can try different activation functions in layer_1, layer_2 and layer_3 like sigmoid or relu. <br>Sometimes choice of activation function affects the results a great deal. In most cases, you will be using non-linear activation functions like tanh, sigmoid or relu.</li>\n",
    "    <li>\n",
    "        <b>metrics=[â€˜accuracyâ€™]:</b> Here default accuracy is used which is categorical_accuracy. Some other accuracy settings are binary_accuracy, sparse_categorical_accuracy etc. Default is the most appropriate here. </li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5 : Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 - 0s - loss: 0.1750 - acc: 0.9667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.17496968805789948, 0.96666664]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_error_rate = model.evaluate(X_test, y_test, verbose=2)\n",
    "test_error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc : 96.67%\n",
      "loss : 17.50%\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "      \"{} : {:.2f}%\".format(model.metrics_names[1],\n",
    "              test_error_rate[1]*100))\n",
    "print(\n",
    "      \"{} : {:.2f}%\".format(model.metrics_names[0],\n",
    "              test_error_rate[0]*100))\n",
    "#You can run the code again to see how the model perform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tanh : acc : 97.78% loss : 33.21% <br>\n",
    "<h5>achieved</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 6 : Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_targets = model.predict_classes(X_test)\n",
    "predicted_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#true_targets = encoder.inverse_transform(y_test.values)\n",
    "#true_targets\n",
    "true_targets = np.argmax(y_test.values, axis=1)\n",
    "true_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flowers in test set: Setosa=10.0 Versicolor=9.0 Virginica=11.0\n"
     ]
    }
   ],
   "source": [
    "#performance_tracker(predicted_targets, true_targets)\n",
    "#flowers = {0:'Setosa', 1:'Versicolor', 2:'Virginica'}\n",
    "print(\"Flowers in test set: Setosa={} Versicolor={} Virginica={}\".format(\n",
    "            y_test.Setosa.sum(), y_test.Versicolor.sum(),\n",
    "            y_test.Virginica.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT: Versicolor predicted as Versicolor\n",
      "CORRECT: Setosa predicted as Setosa\n",
      "CORRECT: Virginica predicted as Virginica\n",
      "CORRECT: Versicolor predicted as Versicolor\n",
      "CORRECT: Versicolor predicted as Versicolor\n",
      "CORRECT: Setosa predicted as Setosa\n",
      "CORRECT: Versicolor predicted as Versicolor\n",
      "CORRECT: Virginica predicted as Virginica\n",
      "CORRECT: Versicolor predicted as Versicolor\n",
      "CORRECT: Versicolor predicted as Versicolor\n",
      "CORRECT: Virginica predicted as Virginica\n",
      "CORRECT: Setosa predicted as Setosa\n",
      "CORRECT: Setosa predicted as Setosa\n",
      "CORRECT: Setosa predicted as Setosa\n",
      "CORRECT: Setosa predicted as Setosa\n",
      "CORRECT: Versicolor predicted as Versicolor\n",
      "CORRECT: Virginica predicted as Virginica\n",
      "CORRECT: Versicolor predicted as Versicolor\n",
      "CORRECT: Versicolor predicted as Versicolor\n",
      "CORRECT: Virginica predicted as Virginica\n",
      "CORRECT: Setosa predicted as Setosa\n",
      "CORRECT: Virginica predicted as Virginica\n",
      "CORRECT: Setosa predicted as Setosa\n",
      "CORRECT: Virginica predicted as Virginica\n",
      "CORRECT: Virginica predicted as Virginica\n",
      "CORRECT: Virginica predicted as Virginica\n",
      "CORRECT: Virginica predicted as Virginica\n",
      "CORRECT: Virginica predicted as Virginica\n",
      "CORRECT: Setosa predicted as Setosa\n",
      "CORRECT: Setosa predicted as Setosa\n"
     ]
    }
   ],
   "source": [
    "for act,exp in zip(predicted_targets, true_targets):\n",
    "        #tup = np.where(exp == 1)\n",
    "        if act != exp: #tup[0][0]\n",
    "            print(\"ERROR: {} predicted as {}\".format(variety[exp],\n",
    "                  variety[act]))\n",
    "        else:\n",
    "            print(\"CORRECT: {} predicted as {}\".format(variety[exp],\n",
    "                  variety[act]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iris = pd.read_csv(\"Iris.csv\") #load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iris.head(2) #show the first 2 rows from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iris.info()  #checking if there is any inconsistency in the dataset\n",
    "#as we see there are no null values in the dataset, so the data can be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fig = iris[iris.variety=='Setosa'].plot(kind='scatter',x='sepal.length',y='sepal.width',color='orange', label='Setosa')\n",
    "# iris[iris.variety=='Versicolor'].plot(kind='scatter',x='sepal.length',y='sepal.width',color='blue', label='Versicolor', ax=fig)\n",
    "# iris[iris.variety=='Virginica'].plot(kind='scatter',x='sepal.length',y='sepal.width',color='green', label='Virginica', ax=fig)\n",
    "# fig.set_xlabel(\"Sepal Length\")\n",
    "# fig.set_ylabel(\"Sepal Width\")\n",
    "# fig.set_title(\"Sepal Length VS Width\")\n",
    "# fig=plt.gcf()\n",
    "# fig.set_size_inches(10,6)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = iris[iris.variety=='Setosa'].plot.scatter(x='petal.length',y='petal.width',color='orange', label='Setosa')\n",
    "# iris[iris.variety=='Versicolor'].plot.scatter(x='petal.length',y='petal.width',color='blue', label='Versicolor',ax=fig)\n",
    "# iris[iris.variety=='Virginica'].plot.scatter(x='petal.length',y='petal.width',color='green', label='Virginica', ax=fig)\n",
    "# fig.set_xlabel(\"Petal Length\")\n",
    "# fig.set_ylabel(\"Petal Width\")\n",
    "# fig.set_title(\" Petal Length VS Width\")\n",
    "# fig=plt.gcf()\n",
    "# fig.set_size_inches(10,6)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris.hist(edgecolor='black', linewidth=1.2)\n",
    "# fig=plt.gcf()\n",
    "# fig.set_size_inches(12,6)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15,10))\n",
    "# plt.subplot(2,2,1)\n",
    "# sns.violinplot(x='variety',y='petal.length',data=iris)\n",
    "# plt.subplot(2,2,2)\n",
    "# sns.violinplot(x='variety',y='petal.width',data=iris)\n",
    "# plt.subplot(2,2,3)\n",
    "# sns.violinplot(x='variety',y='sepal.length',data=iris)\n",
    "# plt.subplot(2,2,4)\n",
    "# sns.violinplot(x='variety',y='sepal.width',data=iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing alll the necessary packages to use the various classification algorithms\n",
    "# from sklearn.linear_model import LogisticRegression  # for Logistic Regression algorithm\n",
    "# from sklearn.model_selection import train_test_split #to split the dataset for training and testing\n",
    "# from sklearn.neighbors import KNeighborsClassifier  # for K nearest neighbours\n",
    "# from sklearn import svm  #for Support Vector Machine (SVM) Algorithm\n",
    "# from sklearn import metrics #for checking the model accuracy\n",
    "# from sklearn.tree import DecisionTreeClassifier #for using Decision Tree Algoithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iris.shape #get the shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(7,4)) \n",
    "# sns.heatmap(iris.corr(),annot=True,cmap='cubehelix_r') #draws  heatmap with input as the correlation matrix calculted by(iris.corr())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = train_test_split(iris, test_size = 0.3)# in this our main data is split into train and test\n",
    "# # the attribute test_size=0.3 splits the data into 70% and 30% ratio. train=70% and test=30%\n",
    "# print(train.shape)\n",
    "# print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X = train[['sepal.length','sepal.width','petal.length','petal.width']]# taking the training data features\n",
    "# train_y=train.variety# output of our training data\n",
    "# test_X= test[['sepal.length','sepal.width','petal.length','petal.width']] # taking test data features\n",
    "# test_y =test.variety  #output value of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train_y.head()  ##output of the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Support Vector Machine (SVM)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = svm.SVC() #select the algorithm\n",
    "# model.fit(train_X,train_y) # we train the algorithm with the training data and the training output\n",
    "# prediction=model.predict(test_X) #now we pass the testing data to the trained algorithm\n",
    "# print('The accuracy of the SVM is:',metrics.accuracy_score(prediction,test_y))#now we check the accuracy of the algorithm. \n",
    "# #we pass the predicted output by the model and the actual output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Logistic Regression</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LogisticRegression()\n",
    "# model.fit(train_X,train_y)\n",
    "# prediction=model.predict(test_X)\n",
    "# print('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction,test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Decision Tree</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=DecisionTreeClassifier()\n",
    "# model.fit(train_X,train_y)\n",
    "# prediction=model.predict(test_X)\n",
    "# print('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction,test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>K-Nearest Neighbours</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=KNeighborsClassifier(n_neighbors=3) #this examines 3 neighbours for putting the new data into a class\n",
    "# model.fit(train_X,train_y)\n",
    "# prediction=model.predict(test_X)\n",
    "# print('The accuracy of the KNN is',metrics.accuracy_score(prediction,test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Let's check the accuracy for various values of n for K-Nearest nerighbours</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_index=list(range(1,11))\n",
    "# a=pd.Series()\n",
    "# x=[1,2,3,4,5,6,7,8,9,10]\n",
    "# for i in list(range(1,11)):\n",
    "#     model=KNeighborsClassifier(n_neighbors=i) \n",
    "#     model.fit(train_X,train_y)\n",
    "#     prediction=model.predict(test_X)\n",
    "#     a=a.append(pd.Series(metrics.accuracy_score(prediction,test_y)))\n",
    "# plt.plot(a_index, a)\n",
    "# plt.xticks(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creating Petals And Sepals Training Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# petal=iris[['petal.length','petal.width','variety']]\n",
    "# sepal=iris[['sepal.length','sepal.width','variety']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_p,test_p=train_test_split(petal,test_size=0.3,random_state=0)  #petals\n",
    "# train_x_p=train_p[['petal.width','petal.length']]\n",
    "# train_y_p=train_p.variety\n",
    "# test_x_p=test_p[['petal.width','petal.length']]\n",
    "# test_y_p=test_p.variety\n",
    "\n",
    "\n",
    "# train_s,test_s=train_test_split(sepal,test_size=0.3,random_state=0)  #Sepal\n",
    "# train_x_s=train_s[['sepal.width','sepal.length']]\n",
    "# train_y_s=train_s.variety\n",
    "# test_x_s=test_s[['sepal.width','sepal.length']]\n",
    "# test_y_s=test_s.variety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>SVM</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=svm.SVC()\n",
    "# model.fit(train_x_p,train_y_p) \n",
    "# prediction=model.predict(test_x_p) \n",
    "# print('The accuracy of the SVM using Petals is:',metrics.accuracy_score(prediction,test_y_p))\n",
    "\n",
    "# model=svm.SVC()\n",
    "# model.fit(train_x_s,train_y_s) \n",
    "# prediction=model.predict(test_x_s) \n",
    "# print('The accuracy of the SVM using Sepal is:',metrics.accuracy_score(prediction,test_y_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Logistic Regression</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LogisticRegression()\n",
    "# model.fit(train_x_p,train_y_p) \n",
    "# prediction=model.predict(test_x_p) \n",
    "# print('The accuracy of the Logistic Regression using Petals is:',metrics.accuracy_score(prediction,test_y_p))\n",
    "\n",
    "# model.fit(train_x_s,train_y_s) \n",
    "# prediction=model.predict(test_x_s) \n",
    "# print('The accuracy of the Logistic Regression using Sepals is:',metrics.accuracy_score(prediction,test_y_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Decision Tree</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=DecisionTreeClassifier()\n",
    "# model.fit(train_x_p,train_y_p) \n",
    "# prediction=model.predict(test_x_p) \n",
    "# print('The accuracy of the Decision Tree using Petals is:',metrics.accuracy_score(prediction,test_y_p))\n",
    "\n",
    "# model.fit(train_x_s,train_y_s) \n",
    "# prediction=model.predict(test_x_s) \n",
    "# print('The accuracy of the Decision Tree using Sepals is:',metrics.accuracy_score(prediction,test_y_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>K-Nearest Neighbours</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=KNeighborsClassifier(n_neighbors=3) \n",
    "# model.fit(train_x_p,train_y_p) \n",
    "# prediction=model.predict(test_x_p) \n",
    "# print('The accuracy of the KNN using Petals is:',metrics.accuracy_score(prediction,test_y_p))\n",
    "\n",
    "# model.fit(train_x_s,train_y_s) \n",
    "# prediction=model.predict(test_x_s) \n",
    "# print('The accuracy of the KNN using Sepals is:',metrics.accuracy_score(prediction,test_y_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
